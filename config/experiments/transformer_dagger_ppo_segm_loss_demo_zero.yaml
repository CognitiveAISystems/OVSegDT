# @package _global_

defaults:
  - /habitat_baselines/rl/auxiliary_losses:
    - segm
  - /habitat_baselines: habitat_baselines_OVON_rl_config_base
  - /habitat_baselines/rl/policy/obs_transforms:
    - resize
    - relabel_teacher_actions
  - ../tasks/objectnav_stretch_hm3d
  - override /habitat/task/actions:
    - stop
    - move_forward
    - turn_left
    - turn_right
  - override /habitat/task/lab_sensors:
    - gps_sensor
    - compass_sensor
    - objnav_explorer
    - clip_objectgoal_sensor
    #- objectgoal_sensor
    - step_id_sensor
    - multi_semantic_sensor

  - override /habitat/task/measurements:
    - distance_to_goal
    - success
    - spl
    - soft_spl
    - collisions
    - collision_penalty

  - _self_

habitat:
  seed: 2025
  environment:
    iterator_options:
      max_scene_repeat_steps: 50000
  task:
    success_reward: 5.0
    slack_reward: -1e-3
    reward_measure: "collision_penalty"
    lab_sensors:
      objnav_explorer:
        map_resolution: 128
        beeline_dist_thresh: 3.5
        visibility_dist: 3.0
        area_thresh: 1.5
        success_distance: 0.25
        fov: 42
      clip_objectgoal_sensor:
        cache: data/text_embeddings/siglip.pkl
    measurements:
      success:
        success_distance: 0.25
      distance_to_goal:
        type: OVONDistanceToGoal
  dataset:
    type: "OVON-v1"
    split: train
    data_path: data/datasets/ovon/hm3d/v1/{split}/{split}.json.gz
  simulator:
    type: "OVONSim-v0"
    navmesh_settings:
      agent_max_climb: 0.1
      cell_height: 0.05
    turn_angle: 30
    action_space_config: "v0noisy"
    action_space_config_arguments:
      NOISE_MODEL:
        robot: "LoCoBot"
        CONTROLLER: "Proportional"
        NOISE_MULTIPLIER: 0.01
    agents:
      main_agent:
        sim_sensors:
          rgb_sensor:
            width: 370
            height: 224
            hfov: 105
            position: [0, 0.8, 0]
            noise_model: "GaussianNoiseModel"
            noise_model_kwargs:
              INTENSITY_CONSTANT: 0.05
          depth_sensor:
            width: 370
            height: 224
            hfov: 105
            position: [0, 0.8, 0]
            min_depth: 0.5
            max_depth: 5.0
          semantic_sensor:
            width: 370
            height: 224
            hfov: 105
            position: [0, 0.8, 0]
        height: 1.41
        radius: 0.17
    habitat_sim_v0:
      gpu_gpu: False
      gpu_device_id: 0
      allow_sliding: False

habitat_baselines:
  torch_gpu_id: 0
  tensorboard_dir: "tb/dagger_ppo_ema_stable_2gpus_dagger"
  video_dir: "video_dir"
  test_episode_count: -1
  eval_ckpt_path_dir: "data/dagger_ppo_ema_stable_2gpus_dagger"
  num_environments: 32
  checkpoint_folder: "data/dagger_ppo_ema_stable_2gpus_dagger"
  trainer_name: "ver_ovon_transformer_custom"
  num_updates: -1
  total_num_steps: 1000000000
  log_interval: 10
  num_checkpoints: 500
  # Force PyTorch to be single threaded as
  # this improves performance considerably
  force_torch_single_threaded: True

  eval:
    split: "val"

  rl:

    policy:
      name: "OVONTransformerPolicy"
      backbone: "siglip"
      fusion_type: "concat"
      use_vis_query: True
      use_residual: True
      residual_vision: True
      rgb_only: False
      segm_mask_input: True
      segmentation_source: "gt"
      segm_loss_enabled: True
      segm_update_config:
        enabled: False
        frequency: 10
      obs_transforms:
        relabel_teacher_actions:
          teacher_label: "objnav_explorer"
      transformer_config:
        model_name: "llama"
        n_layers: 4
        n_heads: 8
        n_hidden: 512
        n_mlp_hidden: 1024
        max_context_length: 100
        shuffle_pos_id_for_update: True

      finetune:
        enabled: True
        lr: 2.5e-4
        start_actor_warmup_at: 20
        start_actor_update_at: 30
        start_critic_warmup_at: 10
        start_critic_update_at: 20

    ppo:
      # ppo params
      clip_param: 0.2
      ppo_epoch: 1
      num_mini_batch: 2
      value_loss_coef: 0.5
      entropy_coef: 0.01
      lr: 2.5e-4
      eps: 1e-5
      max_grad_norm: 0.2
      num_steps: 100
      use_gae: True
      gamma: 0.99
      tau: 0.95
      use_linear_clip_decay: False
      use_linear_lr_decay: True
      reward_window_size: 50

      use_normalized_advantage: False

      hidden_size: 512

    ddppo:
      sync_frac: 0.6
      # The PyTorch distributed backend to use
      distrib_backend: NCCL
      # Visual encoder backbone
      pretrained_weights: ./data/dagger_ppo_ema_stable_2gpus_dagger/ckpt.5.pth
      # Initialize with pretrained weights
      pretrained: True
      # Initialize just the visual encoder backbone with pretrained weights
      pretrained_encoder: False
      # Whether or not the visual encoder backbone will be trained.
      train_encoder: False
      # Whether or not to reset the critic linear layer
      reset_critic: False
