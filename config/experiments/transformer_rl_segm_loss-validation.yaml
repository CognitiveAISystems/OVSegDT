# @package _global_
defaults:
  - /habitat_baselines: habitat_baselines_rl_config_base
  - /habitat_baselines/rl/policy/obs_transforms:
    - resize
  - ../tasks/objectnav_stretch_hm3d
  - override /habitat/task/lab_sensors:
    - clip_objectgoal_sensor
    - step_id_sensor
    - multi_semantic_sensor
  - override /habitat/task/measurements:
    - distance_to_goal
    - success
    - spl
    - soft_spl
    - collisions
    - collision_penalty
  - _self_

habitat:
  # env_task: "GymHabitatEnvV1"
  environment:
    iterator_options:
      max_scene_repeat_steps: 50000
  task:
    success_reward: 5.0
    slack_reward: -1e-3
    reward_measure: "collision_penalty"
    lab_sensors:
      clip_objectgoal_sensor:
        cache: data/text_embeddings/siglip.pkl
    measurements:
      success:
        success_distance: 0.25
      distance_to_goal:
        type: OVONDistanceToGoal
  dataset:
    type: "OVON-v1"
    split: train
    data_path: data/datasets/ovon/hm3d/v1/{split}/{split}.json.gz
  simulator:
    type: "OVONSim-v0"
    navmesh_settings:
      agent_max_climb: 0.1
      cell_height: 0.05
    turn_angle: 30
    agents:
      main_agent:
        sim_sensors:
          rgb_sensor:
            width: 360
            height: 640
            hfov: 42
            position: [0, 1.31, 0]
          depth_sensor:
            width: 360
            height: 640
            hfov: 42
            position: [0, 1.31, 0]
            min_depth: 0.5
            max_depth: 5.0
          semantic_sensor:
            width: 360
            height: 640
            hfov: 42
            position: [0, 1.31, 0]
        height: 1.41
        radius: 0.17
    habitat_sim_v0:
      gpu_gpu: False
      gpu_device_id: 0
      allow_sliding: False

habitat_baselines:
  torch_gpu_id: 0
  tensorboard_dir: "tb/DAgRL_rlft_dcgan_segm_loss_ppo_every_10_1"
  video_dir: "video_dir_seen"
  test_episode_count: -1
  eval_ckpt_path_dir: "data/DAgRL_rlft_dcgan_segm_loss_ppo_every_10_1"
  num_environments: 32
  checkpoint_folder: "data/DAgRL_rlft_dcgan_segm_loss_ppo_every_10_1"
  trainer_name: "ver_transformer"
  num_updates: -1
  total_num_steps: 150000000
  log_interval: 10
  num_checkpoints: 50
  # Force PyTorch to be single threaded as
  # this improves performance considerably
  force_torch_single_threaded: True
  load_resume_state_config: False

  eval:
    split: "val_unseen"

  rl:

    policy:
      name: "OVONTransformerPolicy"
      backbone: "siglip"
      fusion_type: "concat"
      use_vis_query: True
      use_residual: True
      residual_vision: True
      rgb_only: False
      segm_mask_input: True
      segmentation_source: "yolo_val_unseen"
      segm_loss_enabled: False
      segm_update_config:
        enabled: True
        frequency: 10
      transformer_config:
        model_name: "llama"
        n_layers: 4
        n_heads: 8
        n_hidden: 512
        n_mlp_hidden: 1024
        max_context_length: 100
        shuffle_pos_id_for_update: True

      finetune:
        enabled: True
        lr: 1.5e-5
        start_actor_warmup_at: 750
        start_actor_update_at: 1000
        start_critic_warmup_at: 500
        start_critic_update_at: 1000

    ppo:
      # ppo params
      clip_param: 0.2
      ppo_epoch: 1
      num_mini_batch: 2
      value_loss_coef: 0.5
      entropy_coef: 0.01
      lr: 2.5e-4
      eps: 1e-5
      max_grad_norm: 0.2
      num_steps: 100
      use_gae: True
      gamma: 0.99
      tau: 0.95
      use_linear_clip_decay: False
      use_linear_lr_decay: True
      reward_window_size: 50

      use_normalized_advantage: False

      hidden_size: 512

    ddppo:
      sync_frac: 0.6
      # The PyTorch distributed backend to use
      distrib_backend: NCCL
      # Visual encoder backbone
      pretrained_weights: ./data/dagger_segm_loss_segm_input_from_finetune_segm_updates_10_10_no_zeros/ckpt.11.pth
      # Initialize with pretrained weights
      pretrained: False
      # Initialize just the visual encoder backbone with pretrained weights
      pretrained_encoder: False
      # Whether or not the visual encoder backbone will be trained.
      train_encoder: False
      # Whether or not to reset the critic linear layer
      reset_critic: False